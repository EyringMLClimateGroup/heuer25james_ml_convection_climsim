{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d73e7b2-cbce-43b5-98fe-673b9cb8bbf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "import os\n",
    "# os.environ[\"TORCH_CPP_LOG_LEVEL\"]=\"INFO\"\n",
    "# os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, IterableDataset, DataLoader, RandomSampler\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n",
    "import numpy.random as npr\n",
    "from glob import glob\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "# from pytorch_optimizer import SOAP\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# # non-interactive\n",
    "# prog_bar = False\n",
    "# interactive\n",
    "sys.argv[1] = \"1\" # set this to 1 so that if notebook is converted to script, it can actually be used\n",
    "sys.argv[2] = \"0\" # set this to 0 so that if notebook is converted to script, it can actually be used\n",
    "prog_bar = True\n",
    "\n",
    "argv1 = int(sys.argv[1])\n",
    "argv2 = int(sys.argv[2])\n",
    "\n",
    "# SEED = argv1\n",
    "SEED = npr.randint(100)#29#234834#8483#\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"#\"cuda:2\"\n",
    "\n",
    "# DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bfcb359-2430-44a0-ac5c-018b031af44e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_VAL=4000\n",
    "\n",
    "def setup_mlflow_tracking(experiment_name, run_name, params, uri=\"http://127.0.0.1:5000\"):\n",
    "    mlflow.end_run()\n",
    "    mlflow.set_tracking_uri(uri)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    mlflow.pytorch.autolog(log_models=False, log_datasets=False, checkpoint=False)\n",
    "    mlflow.start_run(run_name=run_name)\n",
    "    for k,v in params.items():\n",
    "        mlflow.log_param(str(k), str(v))\n",
    "\n",
    "NUM_EPOCHS = 200\n",
    "# BATCH_SIZE = 512 # 1000 #1024\n",
    "LEARNING_RATE = 1e-3\n",
    "ERR = 1e-6\n",
    "NLEV = 42\n",
    "NLEV_ORIG = 60\n",
    "# # \"engression\"\n",
    "# if argv1 == 0:\n",
    "#     NOISE_LVL = 0.5\n",
    "# elif argv1 == 1:\n",
    "#     NOISE_LVL = 0.75\n",
    "# elif argv1 == 2:\n",
    "#     NOISE_LVL = 1\n",
    "# elif argv1 == 3:\n",
    "#     NOISE_LVL = 2\n",
    "\n",
    "def r2_score(y_pred:torch.Tensor, y_true:torch.Tensor) -> float:\n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = torch.sum((y_true - torch.mean(y_true)) ** 2)\n",
    "    \n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    \n",
    "    return r2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41146daf-ec07-405b-bbae-91b2e43911cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 384*6*3\n",
    "BATCH_SIZE = 256 #1000 #512 #1000\n",
    "SCHEDULER_PATIENCE = 6\n",
    "SCHEDULER_FACTOR = 10**(-0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0ec5c34-968e-4b01-987e-fa0b48a13512",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757\n"
     ]
    }
   ],
   "source": [
    "n_column_features = 18#15\n",
    "n_scalar_features = 1#14#16#4\n",
    "n_features = n_column_features*NLEV+n_scalar_features\n",
    "print(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a362208",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254\n"
     ]
    }
   ],
   "source": [
    "n_column_targets = 6#6\n",
    "n_scalar_targets = 2#8\n",
    "n_targets = n_column_targets*NLEV+n_scalar_targets\n",
    "print(n_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3286b6a6-aeaa-4257-a7d1-57c858a7bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DaskDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        \"\"\"\n",
    "        Initialize with NumPy arrays.\n",
    "        \"\"\"\n",
    "        assert x.shape[0] == y.shape[0], \"Features and labels must have the same number of samples\"\n",
    "        \n",
    "        new_xshape0 = x.shape[0] - x.shape[0]%BATCH_SIZE\n",
    "        x = x[:new_xshape0,:-2].reshape(new_xshape0//BATCH_SIZE, BATCH_SIZE, x.shape[1])\n",
    "        x = x.rechunk((1, BATCH_SIZE, x.shape[1]))\n",
    "\n",
    "        new_yshape0 = y.shape[0] - y.shape[0]%BATCH_SIZE\n",
    "        y = y[:new_yshape0,:].reshape(new_yshape0//BATCH_SIZE, BATCH_SIZE, y.shape[1])\n",
    "        y = y.rechunk((1, BATCH_SIZE, y.shape[1]))\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples.\n",
    "        \"\"\"\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generate one sample of data.\n",
    "        \"\"\"\n",
    "        # chunk_start = index * self.x.chunks[0][0]\n",
    "        # chunk_end = chunk_start + self.x.chunks[0][0]\n",
    "        # x = torch.from_numpy(self.x[chunk_start:chunk_end].compute()).float()\n",
    "        # y = torch.from_numpy(self.y[chunk_start:chunk_end].compute()).float()\n",
    "        x = torch.from_numpy(self.x[index].compute()).float()\n",
    "        y = torch.from_numpy(self.y[index].compute()).float()\n",
    "        # y = torch.tensor(self.y[index]).float()#.to(DEVICE)\n",
    "        x = torch.cat(\n",
    "            [\n",
    "                x[:,:n_column_features*NLEV].reshape(BATCH_SIZE, n_column_features, NLEV).permute(0, 2, 1),\n",
    "                x[:,n_column_features*NLEV:].unsqueeze(1).repeat(1, NLEV, 1).view(BATCH_SIZE, NLEV, n_scalar_features),\n",
    "            ],\n",
    "            -1,\n",
    "        )\n",
    "        # x[:,:n_column_features*NLEV].reshape(BATCH_SIZE, n_column_features, NLEV).permute(0, 2, 1)\n",
    "        \n",
    "        x = x.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d702e34-0afa-4bac-a658-c71fdf0aa890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ablate all 2d variables except pressure (t0,t-1) and clat,slat\n",
    "only_ps_lat_2d_mask = np.ones(1096, dtype=bool)\n",
    "only_ps_lat_2d_mask[1081:1090] = False\n",
    "only_ps_lat_2d_mask[1091:1094] = False\n",
    "no_lat_mask = np.ones(1096, dtype=bool)\n",
    "no_lat_mask[1094:1096] = False\n",
    "only_ps_lat_2d_mask[1081:1090] = False\n",
    "only_3d_mask = np.ones(1096, dtype=bool)\n",
    "only_3d_mask[1080:] = False\n",
    "no_ps_tm1_mask = np.ones(1096, dtype=bool)\n",
    "no_ps_tm1_mask[1091] = False\n",
    "only_ps_mask = np.ones(1096, dtype=bool)\n",
    "only_ps_mask[1081:] = False\n",
    "vertical_inmask = np.ones(1096, dtype=bool)\n",
    "vertical_outmask = np.ones(362, dtype=bool)\n",
    "for i in range(n_column_features):\n",
    "    vertical_inmask[i*NLEV_ORIG:i*NLEV_ORIG+NLEV_ORIG-NLEV] = False\n",
    "for i in range(n_column_targets):\n",
    "    vertical_outmask[i*NLEV_ORIG:i*NLEV_ORIG+NLEV_ORIG-NLEV] = False\n",
    "\n",
    "combined_in_mask = (vertical_inmask & only_ps_mask)\n",
    "# combined_in_mask = (vertical_inmask & no_lat_mask & no_ps_tm1_mask)\n",
    "# combined_in_mask = (vertical_inmask & no_ps_tm1_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7c2cb60-27e6-4bd7-9baa-8126b9037d36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape (10000, 757) (10000, 254)\n",
      "Validaiton shape (10000, 757) (10000, 254)\n"
     ]
    }
   ],
   "source": [
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, x, y, std=0, mask=None):\n",
    "        \"\"\"\n",
    "        Initialize with NumPy arrays.\n",
    "        \"\"\"\n",
    "        assert x.shape[0] == y.shape[0], \"Features and labels must have the same number of samples\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.mask = mask\n",
    "        self.std = std\n",
    "        if self.mask is not None:\n",
    "            print('Attention: Using mask for inputs')\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples.\n",
    "        \"\"\"\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generate one sample of data.\n",
    "        \"\"\"\n",
    "        # Convert the data to tensors when requested\n",
    "        x = torch.from_numpy(self.x[index]).float()#.to(DEVICE)\n",
    "\n",
    "        # Add noise to inputs\n",
    "        # Generate Gaussian noise with mean=0 and std=sqrt(variance)\n",
    "        if self.std > 0:\n",
    "            noise = torch.randn_like(x) * self.std\n",
    "            x = x * (1 + noise)\n",
    "        \n",
    "        # x = torch.tensor(self.x[index]).float()#.to(DEVICE)\n",
    "        if self.mask is not None:\n",
    "            x = x[~self.mask]\n",
    "        y = torch.from_numpy(self.y[index]).float()#.to(DEVICE)\n",
    "        # y = torch.tensor(self.y[index]).float()#.to(DEVICE)\n",
    "        \n",
    "        x = torch.cat(\n",
    "            [\n",
    "                x[:n_column_features*NLEV].reshape(n_column_features, NLEV).permute(1, 0),\n",
    "                x[n_column_features*NLEV:].unsqueeze(1).repeat(NLEV, 1).view(NLEV, n_scalar_features),\n",
    "            ],\n",
    "            -1,\n",
    "        )\n",
    "        # x[:n_column_features*NLEV].reshape(n_column_features, NLEV).permute(1, 0)\n",
    "        \n",
    "        x = x.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "        return x, y\n",
    "\n",
    "# x_train_cs = np.load('/scratch/b/b309215/LEAP/ClimSim_high-res/expandcnv_postprocessed_qcqi/train/train_input_normed_25Msample.npy')\n",
    "x_train_cs = np.load('/scratch/b/b309215/LEAP/ClimSim_high-res/expandcnv_postprocessed_qcqi/train/train_input_normed_10ksample.npy')\n",
    "# x_train_cs = x_train_cs[:,:-2]\n",
    "# x_train_cs = x_train_cs[:,only_ps_lat_2d_mask]\n",
    "# x_train_cs = x_train_cs[:,(vertical_inmask & only_3d_mask)]\n",
    "x_train_cs = x_train_cs[:,combined_in_mask]\n",
    "# x_train_cs = x_train_cs[:,vertical_inmask]\n",
    "# x_train_cs = x_train_cs[:,notpfull_mask]\n",
    "# y_train_cs = np.load('/scratch/b/b309215/LEAP/ClimSim_high-res/expandcnv_postprocessed_qcqi/train/train_target_normed_25Msample.npy')\n",
    "y_train_cs = np.load('/scratch/b/b309215/LEAP/ClimSim_high-res/expandcnv_postprocessed_qcqi/train/train_target_normed_10ksample.npy')\n",
    "y_train_cs = y_train_cs[:,vertical_outmask]\n",
    "\n",
    "# For testing\n",
    "# x_val_cs = np.load('/scratch/b/b309215/LEAP/ClimSim_high-res/expandcnv_postprocessed_qcqi/test/test_input_normed_5Msample.npy')\n",
    "x_val_cs = np.load('/scratch/b/b309215/LEAP/ClimSim_high-res/expandcnv_postprocessed_qcqi/test/test_input_normed_10ksample.npy')\n",
    "# x_val_cs = x_val_cs[:,(vertical_inmask & only_3d_mask)]\n",
    "x_val_cs = x_val_cs[:,combined_in_mask]\n",
    "# x_val_cs = x_val_cs[:,vertical_inmask]\n",
    "# x_val_cs = x_val_cs[:,:-2]\n",
    "# x_val_cs = x_val_cs[:,only_ps_lat_2d_mask]\n",
    "# x_val_cs = x_val_cs[:,notpfull_mask]\n",
    "# # For testing\n",
    "# y_val_cs = np.load('/scratch/b/b309215/LEAP/ClimSim_high-res/expandcnv_postprocessed_qcqi/test/test_target_normed_5Msample.npy')\n",
    "y_val_cs = np.load('/scratch/b/b309215/LEAP/ClimSim_high-res/expandcnv_postprocessed_qcqi/test/test_target_normed_10ksample.npy')\n",
    "y_val_cs = y_val_cs[:,vertical_outmask]\n",
    "# y_val_cs = y_val_cs[:,:]\n",
    "\n",
    "# x_test = np.load('/scratch/b/b309215/LEAP/ClimSim_high-res/expandcnv_postprocessed/test_input_normed.npy')#, mmap_mode='r')\n",
    "# x_test = x_test[:,:-2]\n",
    "# y_test = np.load('/scratch/b/b309215/LEAP/ClimSim_high-res/expandcnv_postprocessed/test_target_normed.npy')#, mmap_mode='r')\n",
    "\n",
    "# npr.seed(4123)\n",
    "# x_train_cs, x_val_cs, y_train_cs, y_val_cs = train_test_split(x_train_cs, y_train_cs, test_size=0.1)\n",
    "print('Train shape', x_train_cs.shape, y_train_cs.shape)\n",
    "print('Validaiton shape', x_val_cs.shape, y_val_cs.shape)\n",
    "# Train shape (3000000, 916) (3000000, 302)\n",
    "# Validaiton shape (1512000, 916) (1512000, 302)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2980560c-5b01-422a-8173-6552767ff043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STEPS_PER_EPOCH = int(np.ceil(x_train_cs.shape[0]/BATCH_SIZE))\n",
    "STEPS_PER_EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c751e05f-d73f-4998-b1f4-02c98566543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = NumpyDataset(x_val_cs, y_val_cs)#, mask=large_scale_forcing_mask)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# - dask -\n",
    "# train_dataset = DaskDataset(x_train_cs, y_train_cs)#, mask=large_scale_forcing_mask)\n",
    "# # train_loader = DataLoader(train_dataset, batch_size=None, shuffle=True, num_workers=24)#, batch_sampler=batch_sampler)\n",
    "# sampler = torch.utils.data.RandomSampler(train_dataset, num_samples=int(1e3))\n",
    "# train_loader = DataLoader(train_dataset, batch_size=None, num_workers=24, sampler=sampler)#, batch_sampler=batch_sampler)\n",
    "# - numpy -\n",
    "train_dataset = NumpyDataset(x_train_cs, y_train_cs, std=0)#, mask=large_scale_forcing_mask)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45b5c4e-fdc0-4b94-af7a-2ca0059f20c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([254]), torch.Size([254]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_MEAN = torch.from_numpy(np.load('/scratch/b/b309215/LEAP/ClimSim_high-res/expandcnv_postprocessed_qcqi/train/train_target_my.npy'))\n",
    "Y_STD = torch.from_numpy(np.load('/scratch/b/b309215/LEAP/ClimSim_high-res/expandcnv_postprocessed_qcqi/train/train_target_sy.npy'))\n",
    "Y_MEAN = Y_MEAN[vertical_outmask]\n",
    "Y_STD = Y_STD[vertical_outmask]\n",
    "Y_MEAN.shape, Y_STD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79bbe4d1-d236-4848-981e-906931feed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add Gaussian noise\n",
    "def add_multiplicative_noise(x, noise_std):\n",
    "    noise = torch.randn_like(x) * noise_std\n",
    "    return x * (1 + noise)\n",
    "    \n",
    "def add_additive_noise(x, noise_std):\n",
    "    noise = torch.randn_like(x) * noise_std\n",
    "    return x + noise\n",
    "\n",
    "# Adaptive noise scheduler class\n",
    "class AdaptiveNoiseScheduler:\n",
    "    def __init__(self, initial_noise_std=0.5, threshold=0.75, increase_factor=1.1, decrease_factor=0.9):\n",
    "        \"\"\"\n",
    "        initial_noise_std: starting standard deviation for the noise.\n",
    "        threshold: performance threshold (e.g., 0.75 accuracy) on validation data.\n",
    "        increase_factor: factor to increase noise_std if performance is above threshold.\n",
    "        decrease_factor: factor to decrease noise_std if performance falls below threshold.\n",
    "        \"\"\"\n",
    "        self.noise_std = initial_noise_std\n",
    "        self.threshold = threshold\n",
    "        self.increase_factor = increase_factor\n",
    "        self.decrease_factor = decrease_factor\n",
    "\n",
    "    def step(self, current_val_metric):\n",
    "        if current_val_metric >= self.threshold:\n",
    "            # Increase noise if performance is above threshold\n",
    "            self.noise_std *= self.increase_factor\n",
    "            # print(f'{current_val_metric} >= {self.threshold} therefore increasing noise')\n",
    "        else:\n",
    "            # Decrease noise if performance falls below threshold\n",
    "            self.noise_std *= self.decrease_factor\n",
    "            # print(f'{current_val_metric} < {self.threshold} therefore decreasing noise')\n",
    "        return self.noise_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e67ddb4-513a-4fdf-a569-131327251562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero_lvl:  0\n"
     ]
    }
   ],
   "source": [
    "zeroout_index = []\n",
    "zero_lvl = NLEV - NLEV_ORIG + 15\n",
    "zero_lvl = zero_lvl if zero_lvl > 0 else 0\n",
    "print('zero_lvl: ', zero_lvl)\n",
    "for i in range(1,6):\n",
    "    zeroout_index.extend(list(range(i*NLEV,i*NLEV+zero_lvl)))\n",
    "\n",
    "zeroout_index = torch.tensor(zeroout_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "595d932b-cc12-4bec-b6f8-6225db8f9df6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def r2_score(y_pred, y_true):\n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = torch.sum((y_true - torch.mean(y_true)) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5ba25ac-7588-4594-84c6-fc33b28c64f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec74b4f0-5787-4ed6-b505-b2d536a01ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# argv2 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8db63-6512-4648-9518-5b685aa6023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if argv2 == 0:\n",
    "    ncorr_lvls=42\n",
    "    vertical_weighting='none'\n",
    "    pinn=False\n",
    "    pinn_weight=None\n",
    "    conservation=False\n",
    "if argv2 == 1:\n",
    "    # ncorr_lvls=35\n",
    "    # vertical_weighting='none'\n",
    "    # pinn=True\n",
    "    # pinn_weight=0.01\n",
    "    # conservation=True\n",
    "    ncorr_lvls=42\n",
    "    vertical_weighting='learned'\n",
    "    pinn=False\n",
    "    pinn_weight=None\n",
    "    conservation=True\n",
    "elif argv2 == 2:\n",
    "    ncorr_lvls=42\n",
    "    vertical_weighting='none'\n",
    "    pinn=True\n",
    "    pinn_weight=0.1\n",
    "    conservation=False\n",
    "elif argv2 == 3:\n",
    "    ncorr_lvls=42\n",
    "    vertical_weighting='none'\n",
    "    pinn=True\n",
    "    pinn_weight=0.3\n",
    "    conservation=False\n",
    "# elif argv2 == 2:\n",
    "#     ncorr_lvls=35\n",
    "#     vertical_weighting='none'\n",
    "#     pinn=True\n",
    "#     pinn_weight=0.25\n",
    "#     conservation=False\n",
    "# elif argv2 == 3:\n",
    "#     ncorr_lvls=35\n",
    "#     vertical_weighting='none'\n",
    "#     pinn=True\n",
    "#     pinn_weight=0.5\n",
    "#     conservation=False\n",
    "\n",
    "# ncorr_lvls=0\n",
    "# vertical_weighting='none'\n",
    "# pinn=False\n",
    "# pinn_weight=None\n",
    "# conservation=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c999e37c-bb2b-4df2-8755-01bac5e04908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_path:  /work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_0.54M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed1_20250415-110358_pinnweightNone_seed1_20250421-104131/val_score=0.9026-best_model.ckpt\n",
      "val_score:  0.9026\n"
     ]
    }
   ],
   "source": [
    "# # for \"engression\"\n",
    "# # save_dirpath=f\"/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/\"\n",
    "\n",
    "# r2_reduction = 0.1#0.03#0.05#\n",
    "# # init_noise = 0.2#0.02#0.07#\n",
    "\n",
    "# # if argv1 == 0:\n",
    "#     # # 0.17 M\n",
    "#     # load_paths = sorted(glob(save_dirpath + '*0.17M*/*.ckpt'), key=lambda x: re.search(r'_(\\d*-\\d*)/', x).group(1))\n",
    "#     # load_path = [p for p in load_paths if 'noise' not in p][-1]\n",
    "#     # val_score = float(re.search(r'/val_score=(\\d\\.\\d*)-', load_path).group(1))\n",
    "# # if argv1 == 1:\n",
    "#     # # 0.54 Model\n",
    "#     # # load_paths = sorted(glob(save_dirpath + '*0.54M*/*.ckpt'), key=lambda x: re.search(r'_(\\d*-\\d*)/', x).group(1))\n",
    "#     # # load_path = [p for p in load_paths if 'noise' not in p][-1]\n",
    "#     # load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_0.54M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed1_20250415-110358_pinnweightNone_seed1_20250421-104131/val_score=0.9026-best_model.ckpt'\n",
    "#     # val_score = float(re.search(r'/val_score=(\\d\\.\\d*)-', load_path).group(1))\n",
    "#     # init_noise = 0.4                                                                                                             > # if argv1 == 2:                                                                                                                   >     # # 0.98M model\n",
    "#     # load_paths = sorted(glob(save_dirpath + '*0.98M*/*.ckpt'), key=lambda x: re.search(r'_(\\d*-\\d*)/', x).group(1))\n",
    "#     # load_path = [p for p in load_paths if 'noise' not in p][-1]\n",
    "#     # val_score = float(re.search(r'/val_score=(\\d\\.\\d*)-', load_path).group(1))\n",
    "# # if argv1 == 3:\n",
    "#     # # 1.6M model\n",
    "#     # load_paths = sorted(glob(save_dirpath + '*1.6M*/*.ckpt'), key=lambda x: re.search(r'_(\\d*-\\d*)/', x).group(1))\n",
    "#     # load_path = [p for p in load_paths if 'noise' not in p][-1]\n",
    "#     # val_score = float(re.search(r'/val_score=(\\d\\.\\d*)-', load_path).group(1))\n",
    "\n",
    "# if argv2 == 0:\n",
    "#     # 0.54 Model\n",
    "#     # load_paths = sorted(glob(save_dirpath + '*0.54M*/*.ckpt'), key=lambda x: re.search(r'_(\\d*-\\d*)/', x).group(1))\n",
    "#     # load_path = [p for p in load_paths if 'noise' not in p][-1]\n",
    "#     load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_0.54M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed1_20250415-110358_pinnweightNone_seed1_20250421-104131/val_score=0.9026-best_model.ckpt'\n",
    "#     # load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_0.54M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed1_20250415-110358_pinnweightNone_seed1_20250421-104131_addnoise-adaptive0.05-init0.07_seed6_20250704-032523/val_score=0.8553-last_model.ckpt'''\n",
    "#     val_score = float(re.search(r'/val_score=(\\d\\.\\d*)-', load_path).group(1))\n",
    "#     init_noise = 0.4\n",
    "# if argv2 == 2:\n",
    "#     # 0.54 Model\n",
    "#     # load_paths = sorted(glob(save_dirpath + '*0.54M*/*.ckpt'), key=lambda x: re.search(r'_(\\d*-\\d*)/', x).group(1))\n",
    "#     # load_path = [p for p in load_paths if 'noise' not in p][-1]\n",
    "#     #alpha=0.01\n",
    "#     # load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_0.54M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed1_20250415-110358_pinnweight0.01_residual-mul385_seed25_20250430-000204/val_score=0.9004-best_model.ckpt'\n",
    "#     #alpha=0.1\n",
    "#     load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_0.54M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed1_20250415-110358_pinnweight0.1_residual-mul385_seed81_20250430-000025/val_score=0.8997-best_model.ckpt'\n",
    "#     #alpha=0.5\n",
    "#     # load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_0.54M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed1_20250415-110358_pinnweight0.5_residual-mul385_seed57_20250430-164854/val_score=0.8896-best_model.ckpt'\n",
    "#     #alpha=0.9\n",
    "#     # load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_0.54M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed1_20250415-110358_pinnweight0.9_residual-mul385_seed83_20250430-230737/val_score=0.6321-best_model.ckpt'\n",
    "#     val_score = float(re.search(r'/val_score=(\\d\\.\\d*)-', load_path).group(1))\n",
    "#     init_noise = 0.2\n",
    "    \n",
    "# print('load_path: ', load_path)\n",
    "# print('val_score: ', val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519f09c-405f-4bdb-990b-80eb6e7f68d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(757,) (757,)\n",
      "(254,)\n",
      "Physical Scales:\n",
      "g: 9.80665 m\n",
      "Time: 10 s\n",
      "rho: 1000.0 K\n",
      "cpd 1004.64 J/kg/K\n",
      "\n",
      "Derived Physical Scales:\n",
      "Length: 980.665 m\n",
      "Energy: 9617.03842225 J/kg\n",
      "Temperature: 9.572621458681716 K\n",
      "Pressure 9617038.422249999 Pa\n",
      "Normalized Constants:\n",
      "alv: 260.0384744449127\n",
      "als: 294.737306387598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HighResLeapModelPhysicsConstraint(\n",
       "  (network): FFNN_LSTM_6_AVG(\n",
       "    (LSTM_1): LSTM(280, 60, num_layers=4, batch_first=True, dropout=0.02, bidirectional=True)\n",
       "    (Linear_1): Linear(in_features=17, out_features=280, bias=True)\n",
       "    (Linear_2): Linear(in_features=640, out_features=120, bias=True)\n",
       "    (Linear_3): Linear(in_features=120, out_features=12, bias=True)\n",
       "    (Linear_4_0): Linear(in_features=120, out_features=240, bias=True)\n",
       "    (Linear_4): Linear(in_features=240, out_features=4, bias=True)\n",
       "    (avg_pool_1): AvgPool1d(kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  )\n",
       "  (criterion): HuberLoss()\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from BiLSTM import HighResLeapModelPhysicsConstraint\n",
    "\n",
    "params = {\n",
    "    \"input_dim\": 25,\n",
    "    \"hidden_dim\": 512,\n",
    "    \"output_dim\": 368,\n",
    "    \"num_layers\": 3,\n",
    "    \"max_epochs\": NUM_EPOCHS,\n",
    "    \"accelerator\": \"gpu\",\n",
    "    \"devices\": [0],#[argv1],#,1,2,3],\n",
    "    # 'data_length' : len(ds_data),\n",
    "    # 'weight_decay' : 2e-3,\n",
    "    # 'dropout' : 0.0\n",
    "}\n",
    "\n",
    "inp_sub = np.loadtxt('/work/bd1179/b309215/shared_e3sm/saved_models/v5/v5cnvqcqi_nols/inp_sub.txt')\n",
    "inp_sub = inp_sub[combined_in_mask]\n",
    "# inp_sub = inp_sub[only_ps_lat_2d_mask]\n",
    "inp_div = np.loadtxt('/work/bd1179/b309215/shared_e3sm/saved_models/v5/v5cnvqcqi_nols/inp_div_qcqifromoutscale.txt')\n",
    "inp_div = inp_div[combined_in_mask]\n",
    "# inp_div = inp_div[only_ps_lat_2d_mask]\n",
    "print(inp_sub.shape, inp_div.shape)\n",
    "# qn_lbd = np.loadtxt('/work/bd1179/b309215/ClimSim/preprocessing/normalizations/inputs/qn_exp_lambda_large.txt', delimiter=',')\n",
    "p_sub = inp_sub[17*NLEV:18*NLEV+1]#[840:901]# +1 because of surface pressure\n",
    "p_div = inp_div[17*NLEV:18*NLEV+1]#[840:901]#\n",
    "# t_sub = inp_sub[0:60]\n",
    "# t_div = inp_div[0:60]\n",
    "# qn_sub = inp_sub[120:180]\n",
    "# qn_div = inp_div[120:180]\n",
    "in_indices = [17,28]#[14]# # p index\n",
    "# out_indices = [0, 60, 120, 180, 240, 300, 360, 361]#[0,0,0,0,0,0,0,0]# # indices of t_phy, qv_phy, qn_phy, u_phy, v_phy, rain, snow in output\n",
    "out_indices = [i*NLEV for i in range(n_column_targets)] + [NLEV*n_column_targets+i for i in range(n_scalar_targets)]\n",
    "out_scale = np.loadtxt('/work/bd1179/b309215/shared_e3sm/saved_models/v5/v5cnv_conv_cons/out_scale.txt')\n",
    "out_scale = out_scale[vertical_outmask]\n",
    "# out_scale = np.loadtxt('/scratch/b/b309215/e3sm-climsim_sandbox.sif/storage/shared_e3sm/saved_models/v5/v5cnv/out_scale.txt')\n",
    "print(out_scale.shape)\n",
    "# norm_constants = [dP_sub, dP_div, t_sub, t_div, qn_sub, qn_div, qn_lbd, out_scale]\n",
    "norm_constants = [p_sub, p_div, out_scale]\n",
    "\n",
    "feature_target_lengths = [n_column_features, n_scalar_features, n_column_targets, n_scalar_targets]\n",
    "\n",
    "# conservation = False #(argv1 == 0 or argv1 == 2)\n",
    "# ncorr_lvls=40\n",
    "# vertical_weighting='none'\n",
    "\n",
    "# load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_3.3M-2_wlatwpmid_nolargescalef_physconstr-35lvl_weightedlearned_residualout_relu-pr_confrelu_precision32_1gpu_5Msamples_seed42_20250227-224221/val_score=0.9082-best_model.ckpt'\n",
    "\n",
    "# # for \"engression\"\n",
    "# noise_scheduler = AdaptiveNoiseScheduler(threshold=val_score-r2_reduction, initial_noise_std=init_noise)\n",
    "# # otherwise\n",
    "# # noise_scheduler = None\n",
    "\n",
    "# noncons\n",
    "# load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_1.6M_wlatwpmid_nophysconstr-0lvl_residualout_relu-pr_confrelu_precision32_1gpu_5Msamples_seed42_20250305-194746/val_score=0.9140-best_model.ckpt'\n",
    "# cons\n",
    "# load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_1.6M_wlatwpmid_physconstr-35lvl_weightedlearned_residualout_relu-pr_confrelu_precision32_1gpu_5Msamples_seed42_20250304-203840/val_score=0.9111-best_model.ckpt'\n",
    "# pinn\n",
    "# load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_1.6M_wlatwpmid_pinn-35lvl_pinnweight0.7_residualout_relu-pr_confrelu_precision32_1gpu_5Msamples_seed42_20250304-203840/val_score=0.9145-best_model.ckpt'\n",
    "# 2dablated\n",
    "# load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_1.6M_wlatwpmid_2dpslat_nophysconstr-35lvl_residualout_relu-pr_confrelu_precision32_1gpu_5Msamples_seed3_20250321-174359/val_score=0.9138-best_model.ckpt'\n",
    "\n",
    "# load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_1.6M_wlatwpmid_42vlvl_nophysconstr-35lvl_residualout_relu-pr_confrelu_precision32_1gpu_5Msamples_seed3_20250329-134334/val_score=0.9044-last_model.ckpt'\n",
    "# load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_1.6M_wlatwpmid_42vlvl_nophysconstr-35lvl_residualout_relu-pr_confrelu_precision32_1gpu_1Msamples_seed3_20250404-124342/val_score=0.8969-last_model.ckpt'\n",
    "# load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_5.1M_wlatwpmid_42vlvl_nophysconstr-35lvl_residualout_relu-pr_confrelu_precision32_1gpu_1Msamples_seed4_20250404-131343/val_score=0.8937-last_model.ckpt'\n",
    "\n",
    "# load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_0.54M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed1_20250415-110358/val_score=0.9005-best_model.ckpt'\n",
    "# load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_0.54M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed12_20250425-095130/val_score=0.9010-best_model.ckpt'\n",
    "# load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_0.99M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed11_20250426-003730/val_score=0.9118-last_model.ckpt'\n",
    "# load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_0.99M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed29_20250426-003638/val_score=0.9119-best_model.ckpt'\n",
    "\n",
    "# load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_0.54M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed1_20250415-110358_pinnweightNone_seed1_20250421-104131/val_score=0.9026-best_model.ckpt'\n",
    "\n",
    "# change for \"engression\"\n",
    "# model = HighResLeapModelPhysicsConstraint.load_from_checkpoint(\n",
    "#     checkpoint_path=load_path,\n",
    "model = HighResLeapModelPhysicsConstraint(#.load_from_checkpoint(\n",
    "    # checkpoint_path=load_path,\n",
    "    # input_size=params[\"input_dim\"],\n",
    "    # output_size=params[\"output_dim\"],\n",
    "    feature_target_lengths=feature_target_lengths,\n",
    "    norm_constants=norm_constants,\n",
    "    zeroout_index=zeroout_index,\n",
    "    in_indices=in_indices,\n",
    "    out_indices=out_indices,\n",
    "    # config=None,\n",
    "    # zeroout_mask=zeroout_mask,\n",
    "    ncorr_lvls=ncorr_lvls,\n",
    "    vertical_weighting=vertical_weighting,\n",
    "    use_confidence=True,\n",
    "    pinn=pinn,\n",
    "    pinn_weight=pinn_weight,\n",
    "    cons_energy=conservation,\n",
    "    cons_mass=conservation,\n",
    "    cons_uv=conservation,\n",
    "    noise_scheduler=noise_scheduler,\n",
    "    nlev=NLEV,\n",
    "    argv1=argv1,\n",
    "    argv2=argv2,\n",
    ")#.to(DEVICE)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fba76e36-126d-43cf-aaf4-6a1c38244e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1.], device='cuda:0', requires_grad=True)\n",
      "tensor([0.3333, 0.3333, 0.3333], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        print(p.numel())\n",
    "        print(p)\n",
    "        relu_weights = F.relu(p)\n",
    "        print(relu_weights/torch.sum(relu_weights))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf6318-81af-4cd6-8c3b-215fe02de8d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540675\n"
     ]
    }
   ],
   "source": [
    "# for p in model.parameters():\n",
    "#     if p.requires_grad:\n",
    "#         print(p.numel())\n",
    "print(count_parameters(model)) # 1162429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7b1c22-8e8a-4bcc-a74c-771311353ac9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 1, 6, 2]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bd1e25e1-f4c1-4f2f-8eb9-7b7acc173cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20250821-140624'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b87d839-dafc-442a-b54e-f6d280313f14",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision=32\n",
    "\n",
    "if conservation and not pinn:\n",
    "    cons_pinn = \"physconstr\"\n",
    "    vertical_weighting_str = f\"_weighted{vertical_weighting}\"\n",
    "elif pinn and not conservation:\n",
    "    cons_pinn = \"pinn\"\n",
    "    vertical_weighting_str = f\"_pinnweight{pinn_weight}\"\n",
    "elif not conservation and not pinn:\n",
    "    cons_pinn = \"nophysconstr\"\n",
    "    vertical_weighting_str = \"\"\n",
    "    \n",
    "if argv1 == 0:\n",
    "    # run_name = f'hr_expandcnv_3.3M-2_wlatwpmid_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_5Msamples_seed{SEED}_{now}'\n",
    "    # run_name = f'hr_expandcnv_0.17M_wlatwpmid_2dpslat_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_5Msamples_seed{SEED}_{now}'\n",
    "    # run_name = f'hr_expandcnv_0.17M_wlatwpmid_18topzeroinout_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_5Msamples_seed{SEED}_{now}'\n",
    "    run_name = f'hr_expandcnv_0.17M_wlatwpmid_{NLEV}vlvl_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_1Msamples_seed{SEED}_{now}'\n",
    "elif argv1 == 1:\n",
    "    # run_name = f'hr_expandcnv_3.3M-2_wlatwpmid_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_5Msamples_seed{SEED}_{now}'\n",
    "    # run_name = f'hr_expandcnv_0.54M_wlatwpmid_2dpslat_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_5Msamples_seed{SEED}_{now}'\n",
    "    # run_name = f'hr_expandcnv_0.54M_wlatwpmid_18topzeroinout_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_5Msamples_seed{SEED}_{now}'\n",
    "    # run_name = f'hr_expandcnv_0.54M_wlatwpmid_{NLEV}vlvl_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_1Msamples_seed{SEED}_{now}'\n",
    "    # run_name = f'hr_expandcnv_0.54M_wlatwpmid_diffloss_{NLEV}vlvl_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_1Msamples_seed{SEED}_{now}'\n",
    "    # run_name = f'hr_expandcnv_0.54M_wolatwpmid_{NLEV}vlvl_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_1Msamples_seed{SEED}_{now}'\n",
    "    # run_name = f'hr_expandcnv_0.54M_wolatwpmid_diffloss_{NLEV}vlvl_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_1Msamples_seed{SEED}_{now}'\n",
    "    # run_name = f'hr_expandcnv_0.54M_wolatwpmid_{NLEV}vlvl_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_1Msamples_seed{SEED}_{now}'\n",
    "    run_name = f'hr_expandcnv_0.54M_2d1-ponlyres_diffloss_{NLEV}vlvl_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_25Msamples_seed{SEED}_{now}'\n",
    "elif argv1 == 2:\n",
    "    # run_name = f'hr_expandcnv_3.3M-2_wlatwpmid_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_5Msamples_seed{SEED}_{now}'\n",
    "    # run_name = f'hr_expandcnv_0.98M_wlatwpmid_2dpslat_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_5Msamples_seed{SEED}_{now}'\n",
    "    # run_name = f'hr_expandcnv_0.98M_wlatwpmid_18topzeroinout_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_5Msamples_seed{SEED}_{now}'\n",
    "    run_name = f'hr_expandcnv_0.98M_wlatwpmid_{NLEV}vlvl_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_1Msamples_seed{SEED}_{now}'\n",
    "elif argv1 == 3:\n",
    "    # run_name = f'hr_expandcnv_3.3M-2_wlatwpmid_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_5Msamples_seed{SEED}_{now}'\n",
    "    # run_name = f'hr_expandcnv_1.6M_wlatwpmid_2dpslat_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_5Msamples_seed{SEED}_{now}'\n",
    "    # run_name = f'hr_expandcnv_1.6M_wlatwpmid_18topzeroinout_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_5Msamples_seed{SEED}_{now}'\n",
    "    # run_name = f'hr_expandcnv_1.6M_wlatwpmid_{NLEV}vlvl_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_1Msamples_seed{SEED}_{now}'\n",
    "    run_name = f'hr_expandcnv_1.6M_2donlyps_{NLEV}vlvl_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_1Msamples_seed{SEED}_{now}'\n",
    "    # run_name = f'hr_expandcnv_1.6M_only3d_{NLEV}vlvl_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_1Msamples_seed{SEED}_{now}'\n",
    "elif argv1 == 4:\n",
    "    run_name = f'hr_expandcnv_5.1M_wlatwpmid_{NLEV}vlvl_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_1Msamples_seed{SEED}_{now}'\n",
    "elif argv1 == 5:\n",
    "    run_name = f'hr_expandcnv_13.6M_wlatwpmid_{NLEV}vlvl_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_1Msamples_seed{SEED}_{now}'\n",
    "elif argv1 == 6:\n",
    "    run_name = f'hr_expandcnv_8M_wlatwpmid_{NLEV}vlvl_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_1Msamples_seed{SEED}_{now}'\n",
    "elif argv1 == 7:\n",
    "    run_name = f'hr_expandcnv_0.88M_2d1-ponlyres_diffloss_{NLEV}vlvl_{cons_pinn}-{ncorr_lvls}lvl{vertical_weighting_str}_residualout_relu-pr_confrelu_precision{precision}_1gpu_25Msamples_seed{SEED}_{now}'\n",
    "    \n",
    "## pinn warm restart\n",
    "#old_run_name = os.path.basename(os.path.dirname(load_path))\n",
    "## run_name = old_run_name + f'_pinnweight{pinn_weight}_seed{SEED}_{now}'\n",
    "## run_name = old_run_name + f'_pinnweight{pinn_weight}_check-residual-magnitude-mul385_seed{SEED}_{now}'\n",
    "#run_name = old_run_name + f'_addnoise-adaptive0.3_seed{SEED}_{now}'\n",
    "\n",
    "# \"engression\"\n",
    "old_run_name = os.path.basename(os.path.dirname(load_path))\n",
    "run_name = old_run_name + f'_addnoise-adaptive{r2_reduction}-init{init_noise}_seed{SEED}_{now}'\n",
    "# # run_name = old_run_name + f'_mulnoise-adaptive0.3_seed{SEED}_{now}'\n",
    "\n",
    "exp_name = \"ClimsimQcQiAllYears\"# \"ClimsimIconMix\"# \"FirstDayOneYear\"\n",
    "setup_mlflow_tracking(experiment_name=exp_name, run_name=run_name, params=params,\n",
    "                     uri='file:///scratch/b/b309215/mlruns')\n",
    "                     # uri='file:///work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/mlruns')\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    # monitor=\"val_score/dataloader_idx_0\", mode=\"max\", patience=3, verbose=True\n",
    "    monitor=\"val_score\",\n",
    "    mode=\"max\",\n",
    "    patience=6,#40,#8,#4,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    # monitor=\"val_score/dataloader_idx_0\",\n",
    "    monitor=\"val_score\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=1,\n",
    "    filename=\"{val_score:.4f}-best_model\",\n",
    "    # dirpath=f\"/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/{run_name}\"\n",
    "    dirpath=f\"/scratch/b/b309215/saved_models/{run_name}\"\n",
    ")\n",
    "\n",
    "# Also save last checkpoint in case of noise training\n",
    "checkpoint_callback_last = ModelCheckpoint(\n",
    "    every_n_epochs=1,   # Saves after every epoch\n",
    "    save_top_k=1,       # Keeps only the most recent model\n",
    "    filename=\"{val_score:.4f}-last_model\",\n",
    "    # dirpath=f\"/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/{run_name}\"\n",
    "    dirpath=f\"/scratch/b/b309215/saved_models/{run_name}\"\n",
    ")\n",
    "\n",
    "lr_monitor_callback = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Trainer(\n",
    "    # accumulate_grad_batches=8,\n",
    "    max_epochs=params[\"max_epochs\"],\n",
    "    accelerator=params[\"accelerator\"],\n",
    "    devices=params[\"devices\"],\n",
    "    enable_checkpointing=True,\n",
    "    precision=precision,#16,#32,#\n",
    "    callbacks=[early_stopping_callback, lr_monitor_callback, checkpoint_callback, checkpoint_callback_last], # change for \"engression\"\n",
    "    enable_progress_bar=prog_bar,\n",
    "    max_time=\"00:04:40:00\",\n",
    "    default_root_dir=\"/scratch/b/b309215/default_root_dir/\",\n",
    "    # detect_anomaly=True,\n",
    "    # callbacks=[early_stopping_callback, lr_monitor_callback],\n",
    "    # strategy='ddp_find_unused_parameters_true'\n",
    ")\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "# torch.save(model.state_dict(), './model_state_dict.pth')tracer_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "efbeceb7-6028-4561-af0c-b4f4c57af2df",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_loss_quantiles(model, load_path):\n",
    "    DEVICE='cuda:0'\n",
    "    \n",
    "    pred_loss = []\n",
    "    true_loss = []\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    for batch in tqdm(val_loader, total=len(val_loader)):\n",
    "        x, y = batch\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "            y_loss = model.criterion(y_pred[...,:n_targets], y)\n",
    "        \n",
    "        pred_loss.append(y_pred[:,n_targets:n_targets*2].mean(dim=-1).detach().cpu().numpy())\n",
    "        true_loss.append(y_loss.mean(dim=-1).detach().cpu().numpy())\n",
    "    \n",
    "    pred_loss = np.concatenate(pred_loss)\n",
    "    true_loss = np.concatenate(true_loss)\n",
    "    \n",
    "    def get_percentiles(data, q0=0, q1=1, n=101):\n",
    "        sorted_data = np.sort(data)\n",
    "        qs = np.linspace(q0, q1, n)\n",
    "        idx_quantiles = (qs*len(sorted_data)).astype(int)\n",
    "        idx_quantiles = np.minimum(idx_quantiles, len(sorted_data)-1)\n",
    "        quantiles = sorted_data[idx_quantiles]\n",
    "        return qs, quantiles\n",
    "    \n",
    "    pred_qs, pred_quantiles = get_percentiles(pred_loss)\n",
    "    true_qs, true_quantiles = get_percentiles(true_loss)\n",
    "    \n",
    "    df = pd.DataFrame(np.stack([true_qs, pred_quantiles, true_quantiles], axis=1), columns=['Quantile', 'True', 'Pred'], )\n",
    "    \n",
    "    # csv_save_path = os.path.join(load_path_dirname, 'loss_quantiles.csv')\n",
    "    csv_save_path = load_path.replace('.ckpt', '-loss_quantiles.csv')\n",
    "    df.to_csv(csv_save_path)\n",
    "    print('loss quantiles saved to: ', csv_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9509c196-91e9-4fdd-9d1d-9f858b6d134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracing\n",
    "from WrapModels import WrappedModelQn, WrappedModelQcQi\n",
    "def save_traced_model(model, load_path):\n",
    "    model.eval()\n",
    "    model.network.LSTM_1.dropout = 0.0\n",
    "\n",
    "    qc_lbd = np.loadtxt('/work/bd1179/b309215/ClimSim/preprocessing/normalizations/inputs/qc_exp_lambda_large.txt', delimiter=',')\n",
    "    qi_lbd = np.loadtxt('/work/bd1179/b309215/ClimSim/preprocessing/normalizations/inputs/qi_exp_lambda_large.txt', delimiter=',')\n",
    "    qc_lbd = qc_lbd[-NLEV:]\n",
    "    qi_lbd = qi_lbd[-NLEV:]\n",
    "    model = model.to_torchscript()\n",
    "    wrapped_model = WrappedModelQcQi(model, inp_sub[:], inp_div[:], out_scale, qc_lbd, qi_lbd, feature_target_lengths[:2], NLEV)\n",
    "    wrapped_model.to('cpu')\n",
    "    test_in = torch.rand((16,n_features), dtype=torch.float32, device='cpu')\n",
    "    traced_wrapped_model = torch.jit.trace(wrapped_model.eval(), test_in)\n",
    "    traced_save_path = load_path.replace('.ckpt', '_meanerr-res-pred_traced_cpu_wrapped_nodo.pt')\n",
    "    traced_wrapped_model.save(traced_save_path)\n",
    "    print('Saving model to: ', traced_save_path)\n",
    "    frozen_traced_wrapped_model = torch.jit.freeze(traced_wrapped_model)\n",
    "    frozen_save_path = load_path.replace('.ckpt', '_meanerr-res-pred_frozen_traced_cpu_wrapped_nodo.pt')\n",
    "    frozen_traced_wrapped_model.save(frozen_save_path)\n",
    "    print('Saving model to: ', frozen_save_path)\n",
    "    optim_frozen_traced_wrapped_model = torch.jit.optimize_for_inference(frozen_traced_wrapped_model)\n",
    "    optim_save_path = load_path.replace('.ckpt', '_meanerr-res-pred_optim_frozen_traced_cpu_wrapped_nodo.pt')\n",
    "    optim_frozen_traced_wrapped_model.save(optim_save_path)\n",
    "    print('Saving model to: ', optim_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5939e43a-ea35-4785-b6c5-916bfd6e9dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 19532/19532 [06:11<00:00, 52.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss quantiles saved to:  /scratch/b/b309215/saved_models/hr_expandcnv_0.54M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed1_20250415-110358_pinnweightNone_seed1_20250421-104131_addnoise-adaptive0.1-init0.4_seed1_20250727-004138/val_score=0.8028-last_model-loss_quantiles.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/WrapModels.py:267: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  x = torch.where(torch.isnan(x), torch.tensor(0.0, device=x.device), x)\n",
      "/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/WrapModels.py:268: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  x = torch.where(torch.isinf(x), torch.tensor(0.0, device=x.device), x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to:  /scratch/b/b309215/saved_models/hr_expandcnv_0.54M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed1_20250415-110358_pinnweightNone_seed1_20250421-104131_addnoise-adaptive0.1-init0.4_seed1_20250727-004138/val_score=0.8028-last_model_meanerr-res-pred_traced_cpu_wrapped_nodo.pt\n",
      "Saving model to:  /scratch/b/b309215/saved_models/hr_expandcnv_0.54M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed1_20250415-110358_pinnweightNone_seed1_20250421-104131_addnoise-adaptive0.1-init0.4_seed1_20250727-004138/val_score=0.8028-last_model_meanerr-res-pred_frozen_traced_cpu_wrapped_nodo.pt\n",
      "Saving model to:  /scratch/b/b309215/saved_models/hr_expandcnv_0.54M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed1_20250415-110358_pinnweightNone_seed1_20250421-104131_addnoise-adaptive0.1-init0.4_seed1_20250727-004138/val_score=0.8028-last_model_meanerr-res-pred_optim_frozen_traced_cpu_wrapped_nodo.pt\n",
      "Physical Scales:\n",
      "g: 9.80665 m\n",
      "Time: 10 s\n",
      "rho: 1000.0 K\n",
      "cpd 1004.64 J/kg/K\n",
      "\n",
      "Derived Physical Scales:\n",
      "Length: 980.665 m\n",
      "Energy: 9617.03842225 J/kg\n",
      "Temperature: 9.572621458681716 K\n",
      "Pressure 9617038.422249999 Pa\n",
      "alv: 260.0384744449127 J/kg\n",
      "als: 294.737306387598 J/kg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 19532/19532 [04:24<00:00, 73.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss quantiles saved to:  /scratch/b/b309215/saved_models/hr_expandcnv_0.54M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed1_20250415-110358_pinnweightNone_seed1_20250421-104131_addnoise-adaptive0.1-init0.4_seed1_20250727-004138/val_score=0.8070-best_model-loss_quantiles.csv\n",
      "Saving model to:  /scratch/b/b309215/saved_models/hr_expandcnv_0.54M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed1_20250415-110358_pinnweightNone_seed1_20250421-104131_addnoise-adaptive0.1-init0.4_seed1_20250727-004138/val_score=0.8070-best_model_meanerr-res-pred_traced_cpu_wrapped_nodo.pt\n",
      "Saving model to:  /scratch/b/b309215/saved_models/hr_expandcnv_0.54M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed1_20250415-110358_pinnweightNone_seed1_20250421-104131_addnoise-adaptive0.1-init0.4_seed1_20250727-004138/val_score=0.8070-best_model_meanerr-res-pred_frozen_traced_cpu_wrapped_nodo.pt\n",
      "Saving model to:  /scratch/b/b309215/saved_models/hr_expandcnv_0.54M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed1_20250415-110358_pinnweightNone_seed1_20250421-104131_addnoise-adaptive0.1-init0.4_seed1_20250727-004138/val_score=0.8070-best_model_meanerr-res-pred_optim_frozen_traced_cpu_wrapped_nodo.pt\n"
     ]
    }
   ],
   "source": [
    "# load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_0.99M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed11_20250426-003730/val_score=0.9118-last_model.ckpt'\n",
    "load_path = '/scratch/b/b309215/saved_models/hr_expandcnv_0.54M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed1_20250415-110358_pinnweightNone_seed1_20250421-104131_addnoise-adaptive0.1-init0.4_seed1_20250727-004138/val_score=0.8028-last_model.ckpt'\n",
    "# load_path = os.path.join(checkpoint_callback_last.dirpath, checkpoint_callback_last.best_model_path)\n",
    "save_loss_quantiles(model, load_path)\n",
    "save_traced_model(model, load_path)\n",
    "\n",
    "# load_path = '/work/bd1179/b309215/ClimSimKaggle/leap-climsim-kaggle-5th/training/save_models/hr_expandcnv_0.99M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed11_20250426-003730/val_score=0.9118-best_model.ckpt'\n",
    "load_path = '/scratch/b/b309215/saved_models/hr_expandcnv_0.54M_2d1-ponlyres_diffloss_42vlvl_nophysconstr-42lvl_residualout_relu-pr_confrelu_precision32_1gpu_25Msamples_seed1_20250415-110358_pinnweightNone_seed1_20250421-104131_addnoise-adaptive0.1-init0.4_seed1_20250727-004138/val_score=0.8070-best_model.ckpt'\n",
    "# load_path = os.path.join(checkpoint_callback.dirpath, checkpoint_callback.best_model_path)\n",
    "model = HighResLeapModelPhysicsConstraint.load_from_checkpoint(\n",
    "    checkpoint_path=load_path,\n",
    "    feature_target_lengths=feature_target_lengths,\n",
    "    norm_constants=norm_constants,\n",
    "    zeroout_index=zeroout_index,\n",
    "    in_indices=in_indices,\n",
    "    out_indices=out_indices,\n",
    "    ncorr_lvls=ncorr_lvls,\n",
    "    vertical_weighting=vertical_weighting,\n",
    "    use_confidence=True,\n",
    "    pinn=pinn,\n",
    "    pinn_weight=pinn_weight,\n",
    "    cons_energy=conservation,\n",
    "    cons_mass=conservation,\n",
    "    cons_uv=conservation,\n",
    "    noise_scheduler=noise_scheduler,\n",
    "    nlev=NLEV,\n",
    "    argv1=argv1,\n",
    "    argv2=argv2,\n",
    ")\n",
    "save_loss_quantiles(model, load_path)\n",
    "save_traced_model(model, load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca37f0e-4242-440b-95ad-49f3fed06086",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Produce error here so that we stop in pyscript mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7552556-30f0-4db1-b48f-3a3265ed679d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Offline R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0dcf80a2-7541-419b-bb6e-f03f9ba58915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5761724d76054ccc8da82f2a46b9a047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">      Validate metric      </span><span style=\"font-weight: bold\">       DataLoader 0        </span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">     masked_val_score      </span><span style=\"color: #800080; text-decoration-color: #800080\">    0.6314232349395752     </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span><span style=\"color: #800080; text-decoration-color: #800080\">   0.030863499268889427    </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">       val_loss_yhat       </span><span style=\"color: #800080; text-decoration-color: #800080\">   4.945286402602278e-10   </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">         val_score         </span><span style=\"color: #800080; text-decoration-color: #800080\">    0.6314232349395752     </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       "\u001b[36m \u001b[0m\u001b[36m    masked_val_score     \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m   0.6314232349395752    \u001b[0m\u001b[35m \u001b[0m\n",
       "\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m  0.030863499268889427   \u001b[0m\u001b[35m \u001b[0m\n",
       "\u001b[36m \u001b[0m\u001b[36m      val_loss_yhat      \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m  4.945286402602278e-10  \u001b[0m\u001b[35m \u001b[0m\n",
       "\u001b[36m \u001b[0m\u001b[36m        val_score        \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m   0.6314232349395752    \u001b[0m\u001b[35m \u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_score': 0.6314232349395752,\n",
       "  'val_loss_yhat': 4.945286402602278e-10,\n",
       "  'val_loss': 0.030863499268889427,\n",
       "  'masked_val_score': 0.6314232349395752}]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model, val_loader)\n",
    "# alpha=0\n",
    "# [{'val_score': 0.895715594291687,\n",
    "#   'val_loss_yhat': 1.411489675140487e-10,\n",
    "#   'val_loss': 0.01829167827963829,\n",
    "#   'masked_val_score': 0.895715594291687}]\n",
    "# alpha=0.01\n",
    "# [{'val_score': 0.8937228918075562,\n",
    "#   'val_loss_yhat': 1.4366670353371802e-10,\n",
    "#   'val_loss': 0.017436325550079346,\n",
    "#   'masked_val_score': 0.8937228918075562}]\n",
    "# alpha=0.1\n",
    "# [{'val_score': 0.8923537135124207,\n",
    "#   'val_loss_yhat': 1.439101476874427e-10,\n",
    "#   'val_loss': 0.018361741676926613,\n",
    "#   'masked_val_score': 0.8923537135124207}]\n",
    "# alpha=0.5\n",
    "# [{'val_score': 0.8837314248085022,\n",
    "#   'val_loss_yhat': 1.574644303836692e-10,\n",
    "#   'val_loss': 0.01707073673605919,\n",
    "#   'masked_val_score': 0.8837314248085022}]\n",
    "# alpha=0.9\n",
    "# [{'val_score': 0.6314232349395752,\n",
    "#   'val_loss_yhat': 4.945286402602278e-10,\n",
    "#   'val_loss': 0.030863499268889427,\n",
    "#   'masked_val_score': 0.6314232349395752}]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (climlab)",
   "language": "python",
   "name": "climlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
